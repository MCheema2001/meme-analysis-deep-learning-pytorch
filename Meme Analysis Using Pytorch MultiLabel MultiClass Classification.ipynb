{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c1409e3",
   "metadata": {},
   "source": [
    "# MusaDAC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c222bd",
   "metadata": {},
   "source": [
    "# Libararies Installation\n",
    "PIP3 FOR **MACOS** and PIP FOR **UBUNTU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01cff48f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: d2l in e:\\anaconda3\\lib\\site-packages (0.17.5)\n",
      "Requirement already satisfied: requests==2.25.1 in e:\\anaconda3\\lib\\site-packages (from d2l) (2.25.1)\n",
      "Requirement already satisfied: jupyter==1.0.0 in e:\\anaconda3\\lib\\site-packages (from d2l) (1.0.0)\n",
      "Requirement already satisfied: numpy==1.21.5 in e:\\anaconda3\\lib\\site-packages (from d2l) (1.21.5)\n",
      "Requirement already satisfied: matplotlib==3.5.1 in e:\\anaconda3\\lib\\site-packages (from d2l) (3.5.1)\n",
      "Requirement already satisfied: pandas==1.2.4 in e:\\anaconda3\\lib\\site-packages (from d2l) (1.2.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\anaconda3\\lib\\site-packages (from requests==2.25.1->d2l) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in e:\\anaconda3\\lib\\site-packages (from requests==2.25.1->d2l) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in e:\\anaconda3\\lib\\site-packages (from requests==2.25.1->d2l) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in e:\\anaconda3\\lib\\site-packages (from requests==2.25.1->d2l) (1.25.11)\n",
      "Requirement already satisfied: jupyter-console in e:\\anaconda3\\lib\\site-packages (from jupyter==1.0.0->d2l) (6.1.0)\n",
      "Requirement already satisfied: qtconsole in e:\\anaconda3\\lib\\site-packages (from jupyter==1.0.0->d2l) (4.7.5)\n",
      "Requirement already satisfied: nbconvert in c:\\users\\ali\\appdata\\roaming\\python\\python38\\site-packages (from jupyter==1.0.0->d2l) (6.5.0)\n",
      "Requirement already satisfied: ipykernel in e:\\anaconda3\\lib\\site-packages (from jupyter==1.0.0->d2l) (5.3.2)\n",
      "Requirement already satisfied: ipywidgets in e:\\anaconda3\\lib\\site-packages (from jupyter==1.0.0->d2l) (7.5.1)\n",
      "Requirement already satisfied: notebook in e:\\anaconda3\\lib\\site-packages (from jupyter==1.0.0->d2l) (6.0.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in e:\\anaconda3\\lib\\site-packages (from matplotlib==3.5.1->d2l) (9.0.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in e:\\anaconda3\\lib\\site-packages (from matplotlib==3.5.1->d2l) (1.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in e:\\anaconda3\\lib\\site-packages (from matplotlib==3.5.1->d2l) (21.3)\n",
      "Requirement already satisfied: cycler>=0.10 in e:\\anaconda3\\lib\\site-packages (from matplotlib==3.5.1->d2l) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in e:\\anaconda3\\lib\\site-packages (from matplotlib==3.5.1->d2l) (2.8.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in e:\\anaconda3\\lib\\site-packages (from matplotlib==3.5.1->d2l) (4.33.3)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in e:\\anaconda3\\lib\\site-packages (from matplotlib==3.5.1->d2l) (2.4.7)\n",
      "Requirement already satisfied: pytz>=2017.3 in e:\\anaconda3\\lib\\site-packages (from pandas==1.2.4->d2l) (2020.1)\n",
      "Requirement already satisfied: ipython in e:\\anaconda3\\lib\\site-packages (from jupyter-console->jupyter==1.0.0->d2l) (7.16.1)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in e:\\anaconda3\\lib\\site-packages (from jupyter-console->jupyter==1.0.0->d2l) (3.0.5)\n",
      "Requirement already satisfied: jupyter-client in e:\\anaconda3\\lib\\site-packages (from jupyter-console->jupyter==1.0.0->d2l) (6.1.6)\n",
      "Requirement already satisfied: pygments in e:\\anaconda3\\lib\\site-packages (from jupyter-console->jupyter==1.0.0->d2l) (2.6.1)\n",
      "Requirement already satisfied: traitlets in c:\\users\\ali\\appdata\\roaming\\python\\python38\\site-packages (from qtconsole->jupyter==1.0.0->d2l) (5.2.1.post0)\n",
      "Requirement already satisfied: ipython-genutils in e:\\anaconda3\\lib\\site-packages (from qtconsole->jupyter==1.0.0->d2l) (0.2.0)\n",
      "Requirement already satisfied: jupyter-core in c:\\users\\ali\\appdata\\roaming\\python\\python38\\site-packages (from qtconsole->jupyter==1.0.0->d2l) (4.10.0)\n",
      "Requirement already satisfied: pyzmq>=17.1 in e:\\anaconda3\\lib\\site-packages (from qtconsole->jupyter==1.0.0->d2l) (19.0.1)\n",
      "Requirement already satisfied: qtpy in e:\\anaconda3\\lib\\site-packages (from qtconsole->jupyter==1.0.0->d2l) (1.9.0)\n",
      "Requirement already satisfied: beautifulsoup4 in e:\\anaconda3\\lib\\site-packages (from nbconvert->jupyter==1.0.0->d2l) (4.9.1)\n",
      "Requirement already satisfied: jinja2>=3.0 in c:\\users\\ali\\appdata\\roaming\\python\\python38\\site-packages (from nbconvert->jupyter==1.0.0->d2l) (3.1.2)\n",
      "Requirement already satisfied: jupyterlab-pygments in e:\\anaconda3\\lib\\site-packages (from nbconvert->jupyter==1.0.0->d2l) (0.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in e:\\anaconda3\\lib\\site-packages (from nbconvert->jupyter==1.0.0->d2l) (2.1.1)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in e:\\anaconda3\\lib\\site-packages (from nbconvert->jupyter==1.0.0->d2l) (0.5.13)\n",
      "Requirement already satisfied: nbformat>=5.1 in c:\\users\\ali\\appdata\\roaming\\python\\python38\\site-packages (from nbconvert->jupyter==1.0.0->d2l) (5.4.0)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in e:\\anaconda3\\lib\\site-packages (from nbconvert->jupyter==1.0.0->d2l) (0.8.4)\n",
      "Requirement already satisfied: defusedxml in e:\\anaconda3\\lib\\site-packages (from nbconvert->jupyter==1.0.0->d2l) (0.6.0)\n",
      "Requirement already satisfied: tinycss2 in c:\\users\\ali\\appdata\\roaming\\python\\python38\\site-packages (from nbconvert->jupyter==1.0.0->d2l) (1.1.1)\n",
      "Requirement already satisfied: bleach in e:\\anaconda3\\lib\\site-packages (from nbconvert->jupyter==1.0.0->d2l) (3.1.5)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in e:\\anaconda3\\lib\\site-packages (from nbconvert->jupyter==1.0.0->d2l) (1.4.2)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in e:\\anaconda3\\lib\\site-packages (from nbconvert->jupyter==1.0.0->d2l) (0.3)\n",
      "Requirement already satisfied: tornado>=4.2 in e:\\anaconda3\\lib\\site-packages (from ipykernel->jupyter==1.0.0->d2l) (6.0.4)\n",
      "Requirement already satisfied: widgetsnbextension~=3.5.0 in e:\\anaconda3\\lib\\site-packages (from ipywidgets->jupyter==1.0.0->d2l) (3.5.1)\n",
      "Requirement already satisfied: prometheus-client in e:\\anaconda3\\lib\\site-packages (from notebook->jupyter==1.0.0->d2l) (0.8.0)\n",
      "Requirement already satisfied: terminado>=0.8.1 in e:\\anaconda3\\lib\\site-packages (from notebook->jupyter==1.0.0->d2l) (0.8.3)\n",
      "Requirement already satisfied: Send2Trash in e:\\anaconda3\\lib\\site-packages (from notebook->jupyter==1.0.0->d2l) (1.5.0)\n",
      "Requirement already satisfied: six in e:\\anaconda3\\lib\\site-packages (from cycler>=0.10->matplotlib==3.5.1->d2l) (1.15.0)\n",
      "Requirement already satisfied: jedi>=0.10 in e:\\anaconda3\\lib\\site-packages (from ipython->jupyter-console->jupyter==1.0.0->d2l) (0.17.1)\n",
      "Requirement already satisfied: pickleshare in e:\\anaconda3\\lib\\site-packages (from ipython->jupyter-console->jupyter==1.0.0->d2l) (0.7.5)\n",
      "Requirement already satisfied: backcall in e:\\anaconda3\\lib\\site-packages (from ipython->jupyter-console->jupyter==1.0.0->d2l) (0.2.0)\n",
      "Requirement already satisfied: decorator in e:\\anaconda3\\lib\\site-packages (from ipython->jupyter-console->jupyter==1.0.0->d2l) (4.4.2)\n",
      "Requirement already satisfied: setuptools>=18.5 in e:\\anaconda3\\lib\\site-packages (from ipython->jupyter-console->jupyter==1.0.0->d2l) (49.2.0.post20200714)\n",
      "Requirement already satisfied: colorama; sys_platform == \"win32\" in e:\\anaconda3\\lib\\site-packages (from ipython->jupyter-console->jupyter==1.0.0->d2l) (0.4.3)\n",
      "Requirement already satisfied: wcwidth in e:\\anaconda3\\lib\\site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->jupyter-console->jupyter==1.0.0->d2l) (0.2.5)\n",
      "Requirement already satisfied: pywin32>=1.0; sys_platform == \"win32\" and platform_python_implementation != \"PyPy\" in e:\\anaconda3\\lib\\site-packages (from jupyter-core->qtconsole->jupyter==1.0.0->d2l) (227)\n",
      "Requirement already satisfied: soupsieve>1.2 in e:\\anaconda3\\lib\\site-packages (from beautifulsoup4->nbconvert->jupyter==1.0.0->d2l) (2.0.1)\n",
      "Requirement already satisfied: nest-asyncio in e:\\anaconda3\\lib\\site-packages (from nbclient>=0.5.0->nbconvert->jupyter==1.0.0->d2l) (1.5.5)\n",
      "Requirement already satisfied: fastjsonschema in c:\\users\\ali\\appdata\\roaming\\python\\python38\\site-packages (from nbformat>=5.1->nbconvert->jupyter==1.0.0->d2l) (2.15.3)\n",
      "Requirement already satisfied: jsonschema>=2.6 in e:\\anaconda3\\lib\\site-packages (from nbformat>=5.1->nbconvert->jupyter==1.0.0->d2l) (3.2.0)\n",
      "Requirement already satisfied: webencodings>=0.4 in e:\\anaconda3\\lib\\site-packages (from tinycss2->nbconvert->jupyter==1.0.0->d2l) (0.5.1)\n",
      "Requirement already satisfied: parso<0.8.0,>=0.7.0 in e:\\anaconda3\\lib\\site-packages (from jedi>=0.10->ipython->jupyter-console->jupyter==1.0.0->d2l) (0.7.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in e:\\anaconda3\\lib\\site-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert->jupyter==1.0.0->d2l) (19.3.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in e:\\anaconda3\\lib\\site-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert->jupyter==1.0.0->d2l) (0.16.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pillow in e:\\anaconda3\\lib\\site-packages (9.0.1)\n",
      "Requirement already satisfied: pytesseract in e:\\anaconda3\\lib\\site-packages (0.3.9)\n",
      "Requirement already satisfied: Pillow>=8.0.0 in e:\\anaconda3\\lib\\site-packages (from pytesseract) (9.0.1)\n",
      "Requirement already satisfied: packaging>=21.3 in e:\\anaconda3\\lib\\site-packages (from pytesseract) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in e:\\anaconda3\\lib\\site-packages (from packaging>=21.3->pytesseract) (2.4.7)\n",
      "Requirement already satisfied: transformers in e:\\anaconda3\\lib\\site-packages (4.19.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in e:\\anaconda3\\lib\\site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: numpy>=1.17 in e:\\anaconda3\\lib\\site-packages (from transformers) (1.21.5)\n",
      "Requirement already satisfied: requests in e:\\anaconda3\\lib\\site-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in e:\\anaconda3\\lib\\site-packages (from transformers) (5.3.1)\n",
      "Requirement already satisfied: filelock in e:\\anaconda3\\lib\\site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: packaging>=20.0 in e:\\anaconda3\\lib\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in e:\\anaconda3\\lib\\site-packages (from transformers) (2020.6.8)\n",
      "Requirement already satisfied: tqdm>=4.27 in e:\\anaconda3\\lib\\site-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in e:\\anaconda3\\lib\\site-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in e:\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\anaconda3\\lib\\site-packages (from requests->transformers) (2020.6.20)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in e:\\anaconda3\\lib\\site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in e:\\anaconda3\\lib\\site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in e:\\anaconda3\\lib\\site-packages (from requests->transformers) (1.25.11)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in e:\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: colorama; platform_system == \"Windows\" in e:\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.3)\n",
      "Requirement already satisfied: timm in e:\\anaconda3\\lib\\site-packages (0.5.4)\n",
      "Requirement already satisfied: torchvision in e:\\anaconda3\\lib\\site-packages (from timm) (0.12.0)\n",
      "Requirement already satisfied: torch>=1.4 in e:\\anaconda3\\lib\\site-packages (from timm) (1.11.0)\n",
      "Requirement already satisfied: typing_extensions in e:\\anaconda3\\lib\\site-packages (from torchvision->timm) (4.1.1)\n",
      "Requirement already satisfied: numpy in e:\\anaconda3\\lib\\site-packages (from torchvision->timm) (1.21.5)\n",
      "Requirement already satisfied: requests in e:\\anaconda3\\lib\\site-packages (from torchvision->timm) (2.25.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in e:\\anaconda3\\lib\\site-packages (from torchvision->timm) (9.0.1)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in e:\\anaconda3\\lib\\site-packages (from requests->torchvision->timm) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in e:\\anaconda3\\lib\\site-packages (from requests->torchvision->timm) (1.25.11)\n",
      "Requirement already satisfied: idna<3,>=2.5 in e:\\anaconda3\\lib\\site-packages (from requests->torchvision->timm) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\anaconda3\\lib\\site-packages (from requests->torchvision->timm) (2020.6.20)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install d2l\n",
    "!pip3 install pillow\n",
    "!pip3 install pytesseract\n",
    "!pip3 install transformers\n",
    "!pip3 install timm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a35efa",
   "metadata": {},
   "source": [
    "# Libraries Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b4a5f04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "#from d2l import torch as d2l\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "import cv2\n",
    "from sklearn import preprocessing\n",
    "import os\n",
    "import numpy as np\n",
    "import warnings\n",
    "import os\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn.functional as F\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0188fa96",
   "metadata": {},
   "source": [
    "# INTIAL PARAMETERS TO USE\n",
    "These Parameter are for optimizing and enabling different features in the notebook such as training loading or using pretrained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59a8494b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bert = False\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6d90eb",
   "metadata": {},
   "source": [
    "# PRE TRAINED BERT MODEL FOR WORD EMBEDDINGS\n",
    "\n",
    "\n",
    "### [Pretrained Model BERT](https://d2l.ai/chapter_natural-language-processing-pretraining/bert-pretraining.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3f0caff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "import torch\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\", padding=True)\n",
    "model_betr = DistilBertModel.from_pretrained(\"distilbert-base-uncased\").to(device)\n",
    "\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "outputs = model_betr(**inputs.to(device))\n",
    "\n",
    "last_hidden_states = outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0ce1167",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_encoding(tokens_a):\n",
    "    spt = \" \". join(str(e) for e in tokens_a)\n",
    "    inputs = tokenizer(spt, return_tensors=\"pt\")\n",
    "    inp = inputs['input_ids']\n",
    "    inp = inp.detach().numpy()[0]\n",
    "    inp = np.pad(inp, [0, 30], mode='constant')\n",
    "    inp = inp[0:30]\n",
    "    inputs['input_ids'] = torch.from_numpy(np.asarray([inp]))\n",
    "    inp = inputs['attention_mask']\n",
    "    inp = inp.detach().numpy()[0]\n",
    "    inp = np.pad(inp, [0, 30], mode='constant', constant_values=1)\n",
    "    inp = inp[0:30]\n",
    "    inputs['attention_mask'] = torch.from_numpy(np.asarray([inp]))\n",
    "    outputs = model_betr(**inputs.to(device))\n",
    "    last_hidden_states = outputs.last_hidden_state\n",
    "    return last_hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b60e050e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 30, 768])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_a = ['a', 'crane', 'is','ss',\"ososS\",\"sss\"]\n",
    "encoded_text = get_bert_encoding(tokens_a)\n",
    "encoded_text.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076acefe",
   "metadata": {},
   "source": [
    "# PRE TRAINED IMAGE EMBEDDINGS FACEBOOK RESNET50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec463655",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DetrFeatureExtractor, DetrForObjectDetection\n",
    "from PIL import Image\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff816678",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "feature_extractor = DetrFeatureExtractor.from_pretrained('facebook/detr-resnet-50')\n",
    "model_resnet = DetrForObjectDetection.from_pretrained('facebook/detr-resnet-50').to(device)\n",
    "\n",
    "inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
    "outputs = model_resnet(**inputs.to(device))\n",
    "\n",
    "# model predicts bounding boxes and corresponding COCO classes\n",
    "logits = outputs.logits\n",
    "bboxes = outputs.pred_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49db6d0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 100, 92])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26aae72",
   "metadata": {},
   "source": [
    "# Reading Meme Labels to Pass In BERT\n",
    "Meme provided will be passed to the BERT to convert into Word Embeddings and will processed Further "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5115fd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_df = pd.read_csv('labels.csv').dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3fbda1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "alllabels = list(l_df['text_corrected'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683678d4",
   "metadata": {},
   "source": [
    "# Image Processing For Model Training and Testing\n",
    "\n",
    "Standardizing the Images Sizes and Dimension and Other Parameters for Meeting Basic Neural Network Requirements\n",
    "\n",
    "Dataset Provided is **memotion_dataset_7k**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "832aacd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c9b9ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_ = list(l_df['image_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "39b1f970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder Already Exists\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Images are Converted in to 150 x 150 and Converting to 3 Channel Image RGB \"\"\"\n",
    "if(os.path.isdir('proc_images') == False):\n",
    "    os.mkdir('proc_images')\n",
    "    image_not = []\n",
    "    for i in tqdm(range(len(images_))):\n",
    "        try:\n",
    "            image = Image.open(\"images/\"+images_[i])\n",
    "            image = image.resize((82,82),Image.ANTIALIAS)\n",
    "            image = image.convert('RGB')\n",
    "            image.save(\"proc_images/\"+images_[i].split('.')[0]+\".png\", format=\"png\")\n",
    "        except:\n",
    "            image_not.append(i)\n",
    "    for i in image_not:\n",
    "        l_df = l_df[l_df['image_name'] != i]\n",
    "else:\n",
    "    print(\"Folder Already Exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01cd2f1",
   "metadata": {},
   "source": [
    "# Image Processing CSV For DataLoader In Pytorch\n",
    "\n",
    "Generate CSV On Desired Columns and Divide Data on Train (60%) , Test (20%) and Validation (20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "43248071",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_d = l_df[['image_name','overall_sentiment','humour','sarcasm','offensive','motivational','text_corrected']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9771a2f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_name 6830\n",
      "overall_sentiment 5\n",
      "humour 4\n",
      "sarcasm 4\n",
      "offensive 4\n",
      "motivational 2\n",
      "text_corrected 6784\n"
     ]
    }
   ],
   "source": [
    "for i in l_d.columns:\n",
    "    print(i,len(l_d[i].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c6cd1689",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(189, 14.772620790629576)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_d['text_corrected'].str.split().str.len().max(),l_d['text_corrected'].str.split().str.len().sum()/len(l_d['text_corrected'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4669626d",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_d['overall_sentiment'] = l_d['overall_sentiment'].replace(\"very_positive\", 1)\n",
    "l_d['overall_sentiment'] = l_d['overall_sentiment'].replace(\"positive\", 1)\n",
    "l_d['overall_sentiment'] = l_d['overall_sentiment'].replace(\"very_negative\", 2)\n",
    "l_d['overall_sentiment'] = l_d['overall_sentiment'].replace(\"negative\", 2)\n",
    "l_d['overall_sentiment'] = l_d['overall_sentiment'].replace(\"neutral\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fbe6034f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['general', 'not_sarcastic', 'twisted_meaning', 'very_twisted'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_d['sarcasm'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ef49a2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_d['humour'] = l_d['humour'].replace(\"not_funny\", 0)\n",
    "l_d['humour'] = l_d['humour'].replace(\"very_funny\", 1)\n",
    "l_d['humour'] = l_d['humour'].replace(\"hilarious\", 1)\n",
    "l_d['humour'] = l_d['humour'].replace(\"funny\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c7b35ea4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['general', 'not_sarcastic', 'twisted_meaning', 'very_twisted'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_d['sarcasm'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "901ee758",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_d['sarcasm'] = l_d['sarcasm'].replace(\"not_sarcastic\", 0)\n",
    "l_d['sarcasm'] = l_d['sarcasm'].replace(\"general\", 1)\n",
    "l_d['sarcasm'] = l_d['sarcasm'].replace(\"twisted_meaning\", 1)\n",
    "l_d['sarcasm'] = l_d['sarcasm'].replace(\"very_twisted\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1fdd1274",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['not_offensive', 'very_offensive', 'slight', 'hateful_offensive'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_d['offensive'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "51d325be",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_d['offensive'] = l_d['offensive'].replace(\"not_offensive\", 0)\n",
    "l_d['offensive'] = l_d['offensive'].replace(\"very_offensive\", 1)\n",
    "l_d['offensive'] = l_d['offensive'].replace(\"slight\", 1)\n",
    "l_d['offensive'] = l_d['offensive'].replace(\"hateful_offensive\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "162c6a8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['not_motivational', 'motivational'], dtype=object)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_d['motivational'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8cf4958f",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_d['motivational'] = l_d['motivational'].replace(\"not_motivational\", 0)\n",
    "l_d['motivational'] = l_d['motivational'].replace(\"motivational\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5a9aeb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_d['image_name'] = [i.split(\".\")[0]+'.png' for i in l_d['image_name'] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c936fbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, validate, test = np.split(l_d.sample(frac=1, random_state=42), [int(.6*len(l_d)), int(.8*len(l_d))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c133a5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv(\"train.csv\",index=False)\n",
    "validate.to_csv(\"validate.csv\",index=False)\n",
    "test.to_csv(\"test.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0229b229",
   "metadata": {},
   "source": [
    "# Data Set Loader in Torch to Feed Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9ca422cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Custom Dataset Loader for Pytorch\n",
    "\n",
    "Batch size taken is 128\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n",
    "\"\"\"\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0].replace(' ',\"\"))\n",
    "        image = read_image(img_path)\n",
    "        label = self.img_labels.iloc[idx, 1]\n",
    "        label2 = self.img_labels.iloc[idx, 2]\n",
    "        label3 = self.img_labels.iloc[idx, 3]\n",
    "        label4 = self.img_labels.iloc[idx, 4]\n",
    "        label5 = self.img_labels.iloc[idx, 5]\n",
    "        text = self.img_labels.iloc[idx, 6]\n",
    "        text = text.split()\n",
    "        inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
    "        outputs = model_resnet(**inputs.to(device))\n",
    "        encoded_text = get_bert_encoding(text)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "       \n",
    "        labelarray = torch.as_tensor(label)\n",
    "        labelarray2 = torch.as_tensor(label2)\n",
    "        labelarray3 = torch.as_tensor(label3)\n",
    "        labelarray4 = torch.as_tensor(label4)\n",
    "        labelarray5 = torch.as_tensor(label5)\n",
    "        return image.float(), labelarray, labelarray2, labelarray3, labelarray4, labelarray5, encoded_text\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7d16f796",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = CustomImageDataset(annotations_file=\"train.csv\",\n",
    "                   img_dir=\"./proc_images\")\n",
    "val_data = CustomImageDataset(annotations_file=\"validate.csv\",\n",
    "                   img_dir=\"./proc_images\")\n",
    "test_data = CustomImageDataset(annotations_file=\"test.csv\",\n",
    "                   img_dir=\"./proc_images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "09a329ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(train_data, batch_size=32)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_data, batch_size=32)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_data, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "407b3b79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data.__getitem__(19)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90b04dc",
   "metadata": {},
   "source": [
    "# Image Neural Network\n",
    "Takes an Input of 20172 Shape Flatten image and Classify 5 Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "15c173b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FirstModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FirstModel, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(20172, 1000),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1000, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 3),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = F.log_softmax(self.linear_relu_stack(x))\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e7f27b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SecondModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SecondModel, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(20172, 1000),\n",
    "            nn.Softmax(),\n",
    "            nn.Linear(1000, 3),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a6e6ce32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThirdModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ThirdModel, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(20172, 5096),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(5096, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2048, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 3),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cdf6cd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Combined_model(nn.Module):\n",
    "    def __init__(self, modelA, modelB, modelC):\n",
    "        super(Combined_model, self).__init__()\n",
    "        self.modelA = modelA\n",
    "        self.modelB = modelB\n",
    "        self.modelC = modelC\n",
    "        self.classifier = nn.Linear(6, 3)\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(3, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2048, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 3),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x1, x2):\n",
    "        x11 = self.modelC(x1)\n",
    "        x2 = self.modelB(x2)\n",
    "        xs = torch.cat((x11, x2), dim=1)\n",
    "        xs = self.classifier(xs)\n",
    "        x3 = self.modelA(x1)\n",
    "        x = torch.cat((xs, x3), dim=1)\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        logits = F.log_softmax(self.linear_relu_stack(x))\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4d19c8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "M1=FirstModel().to(device)\n",
    "M2=SecondModel().to(device)\n",
    "M3=ThirdModel().to(device)\n",
    "model = Combined_model(M1, M2, M3).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896daf2e",
   "metadata": {},
   "source": [
    "# Text Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1e829b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextModel(nn.Module): #For images\n",
    "    def __init__(self):\n",
    "        super(TextModel, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(23040, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 3),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = F.log_softmax(self.linear_relu_stack(x))\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "41f04527",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_model = TextModel().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8769a4b4",
   "metadata": {},
   "source": [
    "# Combining Image and Text Neural Network\n",
    "Image Net and Text Net Combined and then Passed throw ANN of 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c1dc8543",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Combination_model(nn.Module):\n",
    "    def __init__(self, modelA, modelB):\n",
    "        super(Combination_model, self).__init__()\n",
    "        self.modelA = modelA\n",
    "        self.modelB = modelB\n",
    "        self.classifier = nn.Linear(6, 3)\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(3, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2048, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 24),\n",
    "        )\n",
    "        \n",
    "        self.out1 = nn.Linear(24, 3)\n",
    "        self.out2 = nn.Linear(24, 2)\n",
    "        self.out3 = nn.Linear(24, 2)\n",
    "        self.out4 = nn.Linear(24, 2)\n",
    "        self.out5 = nn.Linear(24, 2)\n",
    "        \n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.modelA(x1,x1)\n",
    "        x2 = self.modelB(x2)\n",
    "        x = torch.cat((x1, x2), dim=1)\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        logits = F.log_softmax(self.linear_relu_stack(x))\n",
    "        \n",
    "        out1 = self.out1(logits)\n",
    "        out2 = self.out2(logits)\n",
    "        out3 = self.out3(logits)\n",
    "        out4 = self.out4(logits)\n",
    "        out5 = self.out5(logits)\n",
    "        \n",
    "        return out1, out2, out3, out4, out5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0d10e00a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Combination_model(\n",
       "  (modelA): Combined_model(\n",
       "    (modelA): FirstModel(\n",
       "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "      (linear_relu_stack): Sequential(\n",
       "        (0): Linear(in_features=20172, out_features=1000, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=1000, out_features=512, bias=True)\n",
       "        (3): ReLU()\n",
       "        (4): Linear(in_features=512, out_features=3, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (modelB): SecondModel(\n",
       "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "      (linear_relu_stack): Sequential(\n",
       "        (0): Linear(in_features=20172, out_features=1000, bias=True)\n",
       "        (1): Softmax(dim=None)\n",
       "        (2): Linear(in_features=1000, out_features=3, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (modelC): ThirdModel(\n",
       "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "      (linear_relu_stack): Sequential(\n",
       "        (0): Linear(in_features=20172, out_features=5096, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=5096, out_features=2048, bias=True)\n",
       "        (3): ReLU()\n",
       "        (4): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (5): ReLU()\n",
       "        (6): Linear(in_features=512, out_features=3, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (classifier): Linear(in_features=6, out_features=3, bias=True)\n",
       "    (linear_relu_stack): Sequential(\n",
       "      (0): Linear(in_features=3, out_features=1024, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=1024, out_features=2048, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      (5): ReLU()\n",
       "      (6): Linear(in_features=512, out_features=3, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (modelB): TextModel(\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "    (linear_relu_stack): Sequential(\n",
       "      (0): Linear(in_features=23040, out_features=1024, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=512, out_features=256, bias=True)\n",
       "      (5): ReLU()\n",
       "      (6): Linear(in_features=256, out_features=3, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (classifier): Linear(in_features=6, out_features=3, bias=True)\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): Linear(in_features=3, out_features=1024, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=1024, out_features=2048, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=2048, out_features=512, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=512, out_features=24, bias=True)\n",
       "  )\n",
       "  (out1): Linear(in_features=24, out_features=3, bias=True)\n",
       "  (out2): Linear(in_features=24, out_features=2, bias=True)\n",
       "  (out3): Linear(in_features=24, out_features=2, bias=True)\n",
       "  (out4): Linear(in_features=24, out_features=2, bias=True)\n",
       "  (out5): Linear(in_features=24, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "join_model = Combination_model(model, text_model)\n",
    "join_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f19d92fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 32\n",
    "epochs = 1\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y, y2, y3, y4, y5, z) in enumerate(dataloader):\n",
    "        pred1, pred2, pred3 ,pred4 ,pred5 = join_model(X.to(device),z.to(device))\n",
    "        loss = loss_fn(pred1.to(device), y.to(device))\n",
    "        loss += loss_fn(pred2.to(device), y2.to(device))\n",
    "        loss += loss_fn(pred3.to(device), y3.to(device))\n",
    "        loss += loss_fn(pred4.to(device), y4.to(device))\n",
    "        loss += loss_fn(pred5.to(device), y5.to(device))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch % 2 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "    correct2 = 0\n",
    "    correct3 = 0\n",
    "    correct4 = 0\n",
    "    correct5 = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y, y2, y3, y4, y5, z in dataloader:\n",
    "            pred, pred2, pred3, pred4, pred5 = join_model(X.to(device),z.to(device))\n",
    "            correct += (pred.argmax(1) == y.to(device)).type(torch.float).sum().item()\n",
    "            correct2 += (pred2.argmax(1) == y2.to(device)).type(torch.float).sum().item()\n",
    "            correct3 += (pred3.argmax(1) == y3.to(device)).type(torch.float).sum().item()\n",
    "            correct4 += (pred4.argmax(1) == y4.to(device)).type(torch.float).sum().item()\n",
    "            correct5 += (pred5.argmax(1) == y5.to(device)).type(torch.float).sum().item()\n",
    "    correct /= size\n",
    "    correct2 /= size\n",
    "    correct3 /= size\n",
    "    correct4 /= size\n",
    "    correct5 /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, For overall_sentiment\")\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct2):>0.1f}%, For humour\")\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct3):>0.1f}%, For sarcasm\")\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct4):>0.1f}%, For offensive\")\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct5):>0.1f}%, For motivational\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86daf014",
   "metadata": {},
   "source": [
    "# Training and Testing Joined Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8528e855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 5.084747  [    0/ 4098]\n",
      "loss: 54.339207  [   64/ 4098]\n",
      "loss: 53.433208  [  128/ 4098]\n",
      "loss: 5.979975  [  192/ 4098]\n",
      "loss: 5.524271  [  256/ 4098]\n",
      "loss: 6.125224  [  320/ 4098]\n",
      "loss: 6.288033  [  384/ 4098]\n",
      "loss: 3.679430  [  448/ 4098]\n",
      "loss: 5.052067  [  512/ 4098]\n",
      "loss: 5.723091  [  576/ 4098]\n",
      "loss: 13.444696  [  640/ 4098]\n",
      "loss: 4.266817  [  704/ 4098]\n",
      "loss: 4.084516  [  768/ 4098]\n",
      "loss: 3.690948  [  832/ 4098]\n",
      "loss: 3.907033  [  896/ 4098]\n",
      "loss: 3.609365  [  960/ 4098]\n",
      "loss: 3.621816  [ 1024/ 4098]\n",
      "loss: 3.350447  [ 1088/ 4098]\n",
      "loss: 3.548254  [ 1152/ 4098]\n",
      "loss: 3.080630  [ 1216/ 4098]\n",
      "loss: 3.563564  [ 1280/ 4098]\n",
      "loss: 3.270173  [ 1344/ 4098]\n",
      "loss: 3.287387  [ 1408/ 4098]\n",
      "loss: 3.511893  [ 1472/ 4098]\n",
      "loss: 3.599060  [ 1536/ 4098]\n",
      "loss: 3.265493  [ 1600/ 4098]\n",
      "loss: 3.588131  [ 1664/ 4098]\n",
      "loss: 3.558917  [ 1728/ 4098]\n",
      "loss: 3.328246  [ 1792/ 4098]\n",
      "loss: 3.224560  [ 1856/ 4098]\n",
      "loss: 3.671576  [ 1920/ 4098]\n",
      "loss: 3.330847  [ 1984/ 4098]\n",
      "loss: 3.378232  [ 2048/ 4098]\n",
      "loss: 3.237704  [ 2112/ 4098]\n",
      "loss: 3.276613  [ 2176/ 4098]\n",
      "loss: 3.180121  [ 2240/ 4098]\n",
      "loss: 3.810562  [ 2304/ 4098]\n",
      "loss: 3.117050  [ 2368/ 4098]\n",
      "loss: 3.140370  [ 2432/ 4098]\n",
      "loss: 3.452734  [ 2496/ 4098]\n",
      "loss: 3.329057  [ 2560/ 4098]\n",
      "loss: 3.499299  [ 2624/ 4098]\n",
      "loss: 3.200616  [ 2688/ 4098]\n",
      "loss: 3.153396  [ 2752/ 4098]\n",
      "loss: 3.609713  [ 2816/ 4098]\n",
      "loss: 3.280972  [ 2880/ 4098]\n",
      "loss: 3.720758  [ 2944/ 4098]\n",
      "loss: 3.477858  [ 3008/ 4098]\n",
      "loss: 3.373578  [ 3072/ 4098]\n",
      "loss: 3.388791  [ 3136/ 4098]\n",
      "loss: 3.250529  [ 3200/ 4098]\n",
      "loss: 3.281863  [ 3264/ 4098]\n",
      "loss: 3.231722  [ 3328/ 4098]\n",
      "loss: 3.343502  [ 3392/ 4098]\n",
      "loss: 3.225594  [ 3456/ 4098]\n",
      "loss: 3.429446  [ 3520/ 4098]\n",
      "loss: 3.351556  [ 3584/ 4098]\n",
      "loss: 3.569447  [ 3648/ 4098]\n",
      "loss: 3.170904  [ 3712/ 4098]\n",
      "loss: 3.160262  [ 3776/ 4098]\n",
      "loss: 3.330894  [ 3840/ 4098]\n",
      "loss: 3.248303  [ 3904/ 4098]\n",
      "loss: 3.456871  [ 3968/ 4098]\n",
      "loss: 3.458670  [ 4032/ 4098]\n",
      "loss: 3.154731  [  256/ 4098]\n",
      "Test Error: \n",
      " Accuracy: 59.9%, For overall_sentiment\n",
      "Test Error: \n",
      " Accuracy: 77.0%, For humour\n",
      "Test Error: \n",
      " Accuracy: 77.5%, For sarcasm\n",
      "Test Error: \n",
      " Accuracy: 61.3%, For offensive\n",
      "Test Error: \n",
      " Accuracy: 64.2%, For motivational\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(join_model.parameters(), lr=learning_rate)\n",
    "\n",
    "epochs = 1\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, join_model, loss_fn, optimizer)\n",
    "    test_loop(val_dataloader, join_model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ab7591e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(text_model.state_dict(), \"text.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5bba9b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model.state_dict(), \"img.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1d493422",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(join_model.state_dict(), \"joinedmodel.pt\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6a1969",
   "metadata": {},
   "source": [
    "# Image to Text OCR\n",
    "Convert Image into Text using OCR [pytesseract](https://pyimagesearch.com/2017/07/10/using-tesseract-ocr-python/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9bb7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = pytesseract.image_to_string(Image.open(\"images/image_3.jpg\"))\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1b6cfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbcfe68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
